import requests
import time 
import duckdb
import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from prefect import task, flow
import matplotlib.pyplot as plt
import os

# fetching secret key from our environment. 
# of course i'm not committing it to this repo 
my_api_key = os.environ.get("my_api_key")

# range of years; chosen for a few reasons: 
    # a) it takes quite the while to fetch articles from the API, so i didn't want to do the whole NYT lifespan. also, limit of 500 API calls a day.  
    # b) interpretability; a century is a nice, sensible unit of measure. 
    # c) the television was invented in 1927. starting at 1924 felt logical. 
# year_range = list(range(2024, 2025)
year_range = list(range(1924,2025)) 
month_range=list(range(1,13,6))

# providing our 'base url' (we will iterate through multiple urls; the nyt api has articles sorted by year and month) 
base_url = "https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={my_api_key}"
# path to our database, where our sql tables will be located 
db_path = "nyt_db.duckdb"

# task for loading the relevant data from the api. 
# from experience with this pipeline; any errors are probably completely my fault, but i slid in error handling just in case. 
@task(retries=3, retry_delay_seconds=5, log_prints=True)
def load_relevant_data():
    con = duckdb.connect(db_path)
    # we will not directly include the raw NYT data in our chart, but we need each headline and snippet so that we can perform...
    # ...sentiment analysis on them. 
    # the tool we are using is called vader, hence the sentiment analysis variable score names. 
    # we first need to create an sql table where we can shove all of the nyt data after we fetch it from the api 
    con.execute(f"""
                CREATE TABLE IF NOT EXISTS "NYT_RAW_DATA"(
                    headline VARCHAR,
                    snippet VARCHAR,
                    pubdate TIMESTAMP,
                    vader_score_title DOUBLE,
                    vader_score_snippet DOUBLE
                )
                """)

    # defining our sntiment analysis tool 
    analyzer = SentimentIntensityAnalyzer()

    # creating an empty list in which we can insert rows of data. (both from the api, and generated by us) 
    # "don't you want to put this in an sql table in a permanent database?" well, first we want an easier way for python to deal with it
    rows = []
    # iterating through years, months of the nyt api 
    for year in year_range:
        for month in month_range: 
            print(f"Fetching articles from month {month}, year {year}")

            url = base_url.format(year=year, month=month, my_api_key=my_api_key)

            response = requests.get(url)
            response.raise_for_status()

            payload = response.json()

            docs = payload["response"]["docs"]

            # here we extract certain information about each article that we encounter on the api. 
            # headline, snippet, date
            for article in docs:
                headline = article["headline"].get("main", "")
                snippet = article.get("snippet", "")
                pubdate = article.get("pub_date", "")

               # conducting sentiment analysis on the headline
                vader_score_title = analyzer.polarity_scores(headline)["compound"]
                # conducting sentiment analysis on the snippet
                vader_score_snippet = analyzer.polarity_scores(snippet)["compound"]

                # each article's information is stored into a dictionary. 
                # (information being that which is directly extracted from the api + sentiment analysis) 
                # we put that dictionary into our list of rows of data. 
                rows.append({
                    "headline": headline,
                    "snippet": snippet,
                    "pubdate": pubdate,
                    "vader_score_title": vader_score_title,
                    "vader_score_snippet": vader_score_snippet
                })
        # sleep command because the nyt api does not allow more than 5 api calls a minute. 
        time.sleep(12)

    # turn our rows list of rows into a dataframe so that we can turn it into an sql table in our database
    test_df = pd.DataFrame(rows) 
    # finally turning our dataframe into the long-awaited sql table that goes in the database
    con.execute(f"INSERT INTO NYT_TEST5 SELECT * FROM test_df")
    # 'task completion' notification to make me feel less nervous
    print("Task complete; data loaded into the database!")

# now that the data is in our database, we can mess with it. 
# having permanent databases with permanent datatables instead of just pandas dataframes was useful because...
# ...it's not like i lose all the data that i loaded in there if i stop running a task in this pipeline. 
# that's especially helpful when loading the data in the database in the first place takes such a tremendously long time. 
@task(retries=3, retry_delay_seconds=5, log_prints=True)
def transform_data(): 
    # (every time we run a task in parquet we have to reconnect to the database, we can't just do it globally) 
    con = duckdb.connect(db_path)
    # creating a new data table with data that's actually relevant for our graphs. 
    # we can't plot the sentiments demonstrated by every single article at every single time stamp, so we must look at aggregates. 
    # sql is great because it allows us to transform lots of our data at once like this. 
    con.execute(f"""CREATE TABLE NYT_TEST5_FINAL AS
                SELECT
                DATE_TRUNC('month', pubdate) AS month,
                AVG(vader_score_title) AS mean_vscore_title,
                AVG(vader_score_snippet) AS mean_vscore_snippet
                FROM NYT_TEST3
                GROUP BY DATE_TRUNC('month', pubdate)
                ORDER BY month;
                """)
    # 'task completion' notification to make me feel less nervous
    print("Task complete; table with useable data created.")

# yeah okay fine whatever i'll get to this part
@task(retries=3, retry_delay_seconds=5, log_prints=True)
def generate_chart():
    con = duckdb.connect("nyt_db.duckdb")

    # Load transformed data
    df = con.execute("SELECT * FROM NYT_TEST5_FINAL ORDER BY month").df()

    print(df.head())

    # Plot
    plt.figure(figsize=(10, 5))
    plt.plot(df["month"], df["mean_vscore_title"], label="Title Sentiment")
    plt.plot(df["month"], df["mean_vscore_snippet"], label="Snippet Sentiment")

    plt.xlabel("Month")
    plt.ylabel("Average Sentiment Score")
    plt.title("NYT Sentiment Over Time")
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()

    output_path = "nyt_sentiment_plot.png"
    plt.savefig(output_path)
    print(f"Saved plot to {output_path}")

    return output_path


# if we don't use this, nothing happens. 
@flow
def main():
    load_relevant_data()
    transform_data()
    generate_chart()

# if we don't use this, also nothing happens. 
if __name__ == "__main__":
    main()
